{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLhgq+6Z3NtiDBUI3DypTO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamandakaunda-15/Formative2_HMMs/blob/main/Hidden_Markov_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sLcAviGPIzvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1500519-8956-483a-978b-3005808d51a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hmmlearn\n",
            "  Downloading hmmlearn-0.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.12/dist-packages (from hmmlearn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.12/dist-packages (from hmmlearn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Downloading hmmlearn-0.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hmmlearn\n",
            "Successfully installed hmmlearn-0.3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install hmmlearn scikit-learn\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.fft import fft, fftfreq\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from hmmlearn import hmm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja1f1yieEAh3",
        "outputId": "3b8e51dc-9368-439c-e87d-e2c0ba05a799"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DRIVE_PATH = '/content/drive/MyDrive/Formative_2/HMM_Project_Data_UNPROCESSED/'\n",
        "\n",
        "print(f\"\\nConfiguration complete. Data will be loaded from: {DRIVE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67mCh1zOGGkP",
        "outputId": "72eba8e8-c432-4c5b-d666-1dc16ccc843f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Configuration complete. Data will be loaded from: /content/drive/MyDrive/Formative_2/HMM_Project_Data_UNPROCESSED/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Extraction"
      ],
      "metadata": {
        "id": "DVLtE8pSGRTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Feature Extraction Functions\n",
        "\n",
        "WINDOW_SIZE_HZ = 100 # sampling rate of 100 Hz\n",
        "\n",
        "def extract_features(merged_df):\n",
        "    \"\"\"\n",
        "    Extracts 19 key features from a single merged 10-second sample.\n",
        "    (Time-Domain, Magnitude, Correlation, Frequency-Domain)\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "\n",
        "    accel_cols = ['Acc_x', 'Acc_y', 'Acc_z']\n",
        "    gyro_cols = ['Gyro_x', 'Gyro_y', 'Gyro_z']\n",
        "    all_cols = accel_cols + gyro_cols\n",
        "\n",
        "    df = merged_df.copy()\n",
        "    N = len(df)\n",
        "\n",
        "    if N == 0:\n",
        "        # Handle empty DataFrame case (return zeros)\n",
        "        feature_names = [f'{c}_mean' for c in all_cols] + [f'{c}_std' for c in all_cols] + \\\n",
        "                        ['SMA', 'Acc_xy_corr', 'Acc_xz_corr', 'Acc_yz_corr'] + \\\n",
        "                        [f'{c}_dom_freq' for c in all_cols]\n",
        "        return pd.Series(0.0, index=feature_names)\n",
        "\n",
        "    # 1. Time-Domain Features (Mean, Standard Deviation)\n",
        "    for col in all_cols:\n",
        "        features[f'{col}_mean'] = df[col].mean()\n",
        "        features[f'{col}_std'] = df[col].std()\n",
        "\n",
        "    # 2. Combined/Magnitude Features\n",
        "    df['Acc_Mag'] = np.sqrt(df['Acc_x']**2 + df['Acc_y']**2 + df['Acc_z']**2)\n",
        "    features['SMA'] = df['Acc_Mag'].abs().mean()\n",
        "\n",
        "    # 3. Correlation between Accelerometer Axes\n",
        "    try:\n",
        "        features['Acc_xy_corr'] = pearsonr(df['Acc_x'], df['Acc_y'])[0]\n",
        "        features['Acc_xz_corr'] = pearsonr(df['Acc_x'], df['Acc_z'])[0]\n",
        "        features['Acc_yz_corr'] = pearsonr(df['Acc_y'], df['Acc_z'])[0]\n",
        "    except ValueError:\n",
        "        features['Acc_xy_corr'] = 0.0\n",
        "        features['Acc_xz_corr'] = 0.0\n",
        "        features['Acc_yz_corr'] = 0.0\n",
        "\n",
        "    # 4. Frequency-Domain Features (Dominant Frequency)\n",
        "    for col in all_cols:\n",
        "        signal = df[col].values\n",
        "        yf = fft(signal)\n",
        "        power_spectrum = np.abs(yf[:N//2])**2\n",
        "        xf = fftfreq(N, 1/WINDOW_SIZE_HZ)[:N//2]\n",
        "\n",
        "        # Dominant Frequency (skip DC component)\n",
        "        if len(power_spectrum) > 1:\n",
        "            dominant_freq_index = np.argmax(power_spectrum[1:]) + 1\n",
        "            features[f'{col}_dom_freq'] = xf[dominant_freq_index]\n",
        "        else:\n",
        "            features[f'{col}_dom_freq'] = 0.0\n",
        "\n",
        "    return pd.Series(features)\n",
        "\n",
        "def process_all_samples(base_dir):\n",
        "    \"\"\"Loads, merges, extracts features, and normalizes the entire dataset.\"\"\"\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    activities = ['standing', 'walking', 'jumping', 'still']\n",
        "\n",
        "    for item_name in os.listdir(base_dir):\n",
        "        folder_path = os.path.join(base_dir, item_name)\n",
        "\n",
        "        if os.path.isdir(folder_path):\n",
        "\n",
        "            # Individual Activity Label Extraction\n",
        "            name = item_name.lower()\n",
        "            label = None\n",
        "            for act in activities:\n",
        "                match = re.search(r'^{}|[_-]{}'.format(act, act), name)\n",
        "                if match:\n",
        "                    label = act.capitalize()\n",
        "                    break\n",
        "\n",
        "            if label is None:\n",
        "                continue\n",
        "\n",
        "            #  Loading and Merging CSVs (Accelerometer and Gyroscope) for each activity sample\n",
        "            try:\n",
        "                accel_path = os.path.join(folder_path, 'Accelerometer.csv')\n",
        "                gyro_path = os.path.join(folder_path, 'Gyroscope.csv')\n",
        "\n",
        "                # Load and rename columns\n",
        "                accel_df = pd.read_csv(accel_path).rename(columns={'x': 'Acc_x', 'y': 'Acc_y', 'z': 'Acc_z'})\n",
        "                gyro_df = pd.read_csv(gyro_path).rename(columns={'x': 'Gyro_x', 'y': 'Gyro_y', 'z': 'Gyro_z'})\n",
        "\n",
        "                # MERGING THE 2 CVS into dataframes\n",
        "                merged_df = pd.merge(accel_df, gyro_df, on=['time', 'seconds_elapsed'], how='inner')\n",
        "\n",
        "                # Feature Extraction\n",
        "                features = extract_features(merged_df)\n",
        "\n",
        "                all_features.append(features)\n",
        "                all_labels.append(label)\n",
        "\n",
        "            except FileNotFoundError:\n",
        "                print(f\"Skipping: Missing files in {item_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping: Error processing {item_name}: {e}\")\n",
        "\n",
        "    feature_matrix = pd.DataFrame(all_features)\n",
        "    label_series = pd.Series(all_labels, name='Activity_Label')\n",
        "\n",
        "    # --- Feature Normalization (Z-score standardisation) ---\n",
        "    scaler = StandardScaler()\n",
        "    normalized_features = scaler.fit_transform(feature_matrix)\n",
        "    normalized_df = pd.DataFrame(normalized_features, columns=feature_matrix.columns)\n",
        "\n",
        "    return normalized_df, label_series"
      ],
      "metadata": {
        "id": "WJPbc_6fGQav"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the full processing pipeline\n",
        "feature_matrix, activity_labels = process_all_samples(DRIVE_PATH)\n",
        "\n",
        "print(f\"Successfully processed {len(feature_matrix)} total samples.\")\n",
        "print(\"Feature Matrix Shape:\", feature_matrix.shape)\n",
        "print(\"\\nActivity Distribution:\")\n",
        "print(activity_labels.value_counts())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQF5mnNsHEHi",
        "outputId": "d8ab41ed-c72b-4479-f347-858716cbeff4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 51 total samples.\n",
            "Feature Matrix Shape: (51, 22)\n",
            "\n",
            "Activity Distribution:\n",
            "Activity_Label\n",
            "Walking     16\n",
            "Standing    12\n",
            "Still       12\n",
            "Jumping     11\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Model Components"
      ],
      "metadata": {
        "id": "v3cFdlQHJvaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Training and Test Data (for Tasks 4 & 5)\n",
        "# a 70/30 split for training/testing.\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    feature_matrix, activity_labels, test_size=0.3, random_state=42, stratify=activity_labels\n",
        ")\n"
      ],
      "metadata": {
        "id": "v9IY19mjHbBp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to numerical format (required by hmmlearn)\n",
        "label_map = {name: i for i, name in enumerate(activity_labels.unique())}\n",
        "y_train_num = y_train.map(label_map).values\n",
        "y_test_num = y_test.map(label_map).values\n",
        "\n",
        "# Inputing a a single sequence (or list of sequences) for a Hidden Markov Model\n",
        "X_train_sequence = X_train.values\n",
        "\n",
        "# Store the final objects for the next steps\n",
        "DATA = {\n",
        "    'X_train': X_train_sequence,\n",
        "    'X_test': X_test.values,\n",
        "    'y_test_num': y_test_num,\n",
        "    'label_map': label_map,\n",
        "    'reverse_label_map': {v: k for k, v in label_map.items()}\n",
        "}"
      ],
      "metadata": {
        "id": "3QOb6n7xHgoD"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}